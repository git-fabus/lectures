


\subsection{Kondition und Stabilität}
Gegeben sei eine stetige und differenzierbare Funktion
\[
f \colon \R \to \R, x \mapsto y = f(x) 
\]
Für fehlerhafte Daten $x+\Delta x$ mit kleinen Fehler $\Delta x$ gilt:
\[
\frac{f(x+\Delta x)-f(x)}{\Delta x} \approx f'(x)\text{.}
\]
Für den absoluten Datenfehler $\Delta y$ gilt:
\[
\Delta y= f(x+\Delta x)-f(x) \approx f'(x) \cdot \Delta x
\]
und für den relativen Fehler:
\[
\frac{\Delta y}{y}= \frac{f'(x)\cdot \Delta x}{f(x)}=\frac{f'(x)x}{f(x)}\cdot \frac{\Delta x}{x}
\]
\begin{definition}[Konditionszahlen]
\label{def:konditionszahl}
Die Zahl \[
K_{abs} = |f'(x)|\]
heißt \emph{absolute Konditionszahl} des Problems $x \mapsto f(x)$. 
Für $f(x)\cdot x \neq 0$ heißt
\[
K_{rel}=|\frac{f'(x) \cdot x}{f\left( x \right)}|\] 
die entsprechende \emph{relative Konditionszahl}.
Ein Problem heißt \underline{schlecht konditioniert},falls eine dieser beiden Konditionszahlen deutlich größer als 1 sind.
Ansonsten heißt das Problem \underline{gut konditioniert}.
\end{definition}
\begin{example}
Konditionierung der Addition und Multiplikation
\begin{itemize}
	\item Für die Addition $f(x)=x+a$ gilt
\[
K_{rel}= |\frac{f'(x)x}{f(x)}|= |\frac{x}{x+1}|
\]
$\implies K_{rel}$ ist groß, falls $|x+a| \ll |x|$.
\item Für die Multiplikation $f(x)=x \cdot a$ gilt
\[
K_{rel} = \frac{|f'(x)x|}{|f(x)|}= \frac{|ax|}{|ax|}=1
\]
$\implies$ Die absolute Kondition ist schlecht, falls $1 \ll q$. Die relative Kondition ist immer gut.
\end{itemize}
\end{example}
\begin{definition}
Erfüllt die Implementierung eines Algorithmus $\boxed{f}$ zur Lösung eines Problems $x \mapsto f(x)$ die Abschätzung
\[
|\frac{\boxed{f}-f(x)}{f(x)} \le C_V K_{rel} \varepsilon_{mach}
\]
mit einem mäßig großen $C_V >0$, so wird der Algorithmus \emph{vorwärtsstabil} genannt.
Ergibt die Rückwärtsanalyse $\boxed{f}(x)=f(x+\Delta x)$ mit
\[
|\frac{\Delta x}{x} \le C_R \varepsilon_{mach}
\]
mit $C_R>0$ nicht zu groß, so heißt der Algorithmus \boxed{f} \emph{rückwärtsstabil}

%Fancy Grafik Vorlesung 5 Seite 4
\end{definition}
\begin{theorem}
Jeder rückwärtsstabile Algorithmus ist auch vorwärtsstabil
\end{theorem}
\begin{proof}
Übung.
\end{proof}
Oft ist Rückwärtsstabilität einfacher nachzuweisen.
\paragraph{Faustregel}zu konditionierten Probleme
\begin{itemize}
	\item Gut konditioniertes Problem + stabiler Algorithmus \\ $\implies$ Gute numerische Resultate.
\item Schlecht konditioniertes Problem oder instabiler Algorithmus \\ $\implies$ Fragwürdige Ergebnisse.
\end{itemize}

\begin{example}
Fortsetzung von Beispiel \ref{eg:rückwärtsanalyse} \\
Die Rückwärtsanalyse hat gezeigt: Falls $0<|q| \ll 1 <p$ wird der numerische Fehler untragbar.
Unsere Abbildung ist:
\[
f(q)=p-\sqrt[2]{p^2-1}
\]
Als Konditionszahlen ergeben sich:
\begin{equation*}
K_{abs} = |f'(q)|= |\frac{1}{2\sqrt[2]{p^2-q}}| < 1
\end{equation*}
\begin{align*}
	K_{rel} 
	&=|\frac{f'(q)q}{f(q)}| \\
	&=|\frac{q}{2\sqrt[2]{p^2-q}(p-\sqrt[2]{p^2-q})(p+\sqrt[2]{p^2-q})}| \\
	&=\frac{1}{2}|\frac{p+\sqrt[2]{p^2-q}}{\sqrt[2]{p^2-q}} \\
	&\approx \frac{1}{2}|\frac{p+p}{p}| \approx 1
\end{align*}%
$\implies$ Nullstellenberechung ist ein gut konditioniertes Problem, aber  Algorithmus \ref{alg:nst1} muss instabil sein
\end{example}

